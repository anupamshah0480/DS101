Random Forest is a flexible, easy to use machine learning algorithm that produces, even without hyper-parameter tuning, 
a great result most of the time. It is also one of the most used algorithms, because it’s simplicity and the fact that it 
can be used for both classification and regression tasks.

To say it in simple words: Random forest builds multiple decision trees and merges them together to get a more accurate
and stable prediction.

Random Forest adds additional randomness to the model, while growing the trees. Instead of searching for the most important 
feature while splitting a node, it searches for the best feature among a random subset of features. This results in a wide 
diversity that generally results in a better model.

Random Forest is a great algorithm to train early in the model development process, to see how it performs and it’s hard to 
build a “bad” Random Forest, because of its simplicity. This algorithm is also a great choice, if you need to develop a model 
in a short period of time. On top of that, it provides a pretty good indicator of the importance it assigns to your features.

Random Forests are also very hard to beat in terms of performance. Of course you can probably always find a model that can 
perform better, like a neural network, but these usually take much more time in the development. And on top of that, they 
can handle a lot of different feature types, like binary, categorical and numerical.

Overall, Random Forest is a (mostly) fast, simple and flexible tool, although it has its limitations.

Source - Towards Data Science


